{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# AIG230 Natural Language Processing - Assignment 5\n",
                "\n",
                "**Selected Corpus: Option A - Gutenberg ('carroll-alice.txt')**\n",
                "\n",
                "**Student Name:** [Your Name]\n",
                "**Date:** February 16, 2026\n",
                "**Instructor:** Prof: David Quispe"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part A - Text Preprocessing (50%)\n",
                "\n",
                "**Goal:** Prepare a clean token stream and justify your choices."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package gutenberg to\n",
                        "[nltk_data]     /Users/kevinwang/nltk_data...\n",
                        "[nltk_data]   Package gutenberg is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt to /Users/kevinwang/nltk_data...\n",
                        "[nltk_data]   Package punkt is already up-to-date!\n",
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     /Users/kevinwang/nltk_data...\n",
                        "[nltk_data]   Package stopwords is already up-to-date!\n",
                        "[nltk_data] Downloading package wordnet to\n",
                        "[nltk_data]     /Users/kevinwang/nltk_data...\n",
                        "[nltk_data]   Package wordnet is already up-to-date!\n",
                        "[nltk_data] Downloading package punkt_tab to\n",
                        "[nltk_data]     /Users/kevinwang/nltk_data...\n",
                        "[nltk_data]   Package punkt_tab is already up-to-date!\n",
                        "[nltk_data] Downloading package omw-1.4 to\n",
                        "[nltk_data]     /Users/kevinwang/nltk_data...\n",
                        "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 1,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import nltk\n",
                "from nltk.corpus import gutenberg\n",
                "import string\n",
                "from collections import Counter\n",
                "import pandas as pd\n",
                "\n",
                "# Download necessary resources\n",
                "nltk.download('gutenberg')\n",
                "nltk.download('punkt')\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "nltk.download('punkt_tab')\n",
                "nltk.download('omw-1.4')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### A1. Load the corpus"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total number of characters: 144395\n",
                        "Total number of sentences: 1703\n",
                        "Total number of tokens BEFORE preprocessing: 34110\n"
                    ]
                }
            ],
            "source": [
                "# Load Alice in Wonderland\n",
                "raw_text = gutenberg.raw('carroll-alice.txt')\n",
                "sentences = gutenberg.sents('carroll-alice.txt')\n",
                "initial_tokens = gutenberg.words('carroll-alice.txt')\n",
                "\n",
                "print(f\"Total number of characters: {len(raw_text)}\")\n",
                "print(f\"Total number of sentences: {len(sentences)}\")\n",
                "print(f\"Total number of tokens BEFORE preprocessing: {len(initial_tokens)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### A2. Preprocess\n",
                "\n",
                "I will implement a preprocessing function that performs:\n",
                "* Lowercasing\n",
                "* Tokenization\n",
                "* Removal of punctuation tokens\n",
                "* Stopword removal (optional, but I'll justify its use for vectorization tasks while keeping it optional for embeddings if needed later)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Total number of tokens AFTER preprocessing: 12240\n",
                        "Vocabulary size (unique tokens): 2240\n",
                        "\n",
                        "Top 20 most frequent tokens:\n",
                        "said: 462\n",
                        "alice: 398\n",
                        "little: 128\n",
                        "one: 105\n",
                        "know: 90\n",
                        "like: 86\n",
                        "would: 83\n",
                        "went: 83\n",
                        "thing: 80\n",
                        "could: 77\n",
                        "time: 77\n",
                        "thought: 76\n",
                        "queen: 76\n",
                        "see: 67\n",
                        "king: 64\n",
                        "well: 63\n",
                        "turtle: 61\n",
                        "head: 60\n",
                        "began: 58\n",
                        "way: 57\n"
                    ]
                }
            ],
            "source": [
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "\n",
                "stop_words = set(stopwords.words('english'))\n",
                "lemmatizer = WordNetLemmatizer()\n",
                "\n",
                "def preprocess_text(text_tokens, remove_stopwords=True, use_lemmatization=True):\n",
                "    # Lowercasing and removing punctuation\n",
                "    clean_tokens = [t.lower() for t in text_tokens if t not in string.punctuation and t.isalpha()]\n",
                "    \n",
                "    if remove_stopwords:\n",
                "        clean_tokens = [t for t in clean_tokens if t not in stop_words]\n",
                "        \n",
                "    if use_lemmatization:\n",
                "        clean_tokens = [lemmatizer.lemmatize(t) for t in clean_tokens]\n",
                "        \n",
                "    return clean_tokens\n",
                "\n",
                "processed_tokens = preprocess_text(initial_tokens)\n",
                "\n",
                "print(f\"Total number of tokens AFTER preprocessing: {len(processed_tokens)}\")\n",
                "print(f\"Vocabulary size (unique tokens): {len(set(processed_tokens))}\")\n",
                "\n",
                "top_20 = Counter(processed_tokens).most_common(20)\n",
                "print(\"\\nTop 20 most frequent tokens:\")\n",
                "for token, count in top_20:\n",
                "    print(f\"{token}: {count}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### A3. Reflection\n",
                "\n",
                "Preprocessing is a critical step in NLP that directly influences the quality and interpretability of downstream models. Lowercasing ensures that words like 'Alice' and 'alice' are treated as the same token, reducing vocabulary sparsity. Removing punctuation eliminates noise that doesn't carry semantic meaning for most bag-of-words tasks. Stopword removal filters out high-frequency but low-information words like 'the' or 'and', which is essential for TF-IDF to highlight uniquely descriptive terms in a document. Lemmatization further reduces vocabulary size by mapping inflected forms (e.g., 'running', 'ran') to their base dictionary form ('run'), helping the model generalize across different grammatical usages. However, for word embeddings, keeping stopwords and sentence structure is often beneficial as they provide context for semantic relationships. These choices trade off between reducing computational complexity and preserving fine-grained linguistic nuances."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part B - Text Representation (25%)\n",
                "\n",
                "**Goal:** Compare Bag-of-Words and TF-IDF representations and interpret the results."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### B1. Create documents\n",
                "\n",
                "I will use each sentence as a document, as it provides a manageable size for similarity analysis and interpretation in this corpus. Using sentences is justified because it allows us to analyze the similarity of specific narrative lines rather than large thematic chapters, which makes the TF-IDF terms easier to interpret."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Number of documents (sentences): 1290\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "import numpy as np\n",
                "\n",
                "# Re-process tokens at sentence level for vectorization\n",
                "def preprocess_sentence(sent_tokens):\n",
                "    return \" \".join(preprocess_text(sent_tokens))\n",
                "\n",
                "documents = [preprocess_sentence(s) for s in sentences if len(preprocess_text(s)) > 2] # Filter tiny sentences\n",
                "print(f\"Number of documents (sentences): {len(documents)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### B2. Vectorize"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "BoW matrix shape: (1290, 2195)\n",
                        "TF-IDF matrix shape: (1290, 2195)\n",
                        "\n",
                        "Top TF-IDF terms for Document 10:\n",
                        "[('stair', np.float64(0.47394270113116527)), ('tumbling', np.float64(0.47394270113116527)), ('fall', np.float64(0.4081414539484156)), ('shall', np.float64(0.3372936658722457)), ('nothing', np.float64(0.3130742328638693)), ('think', np.float64(0.2800352846186907)), ('thought', np.float64(0.2655639655012973)), ('alice', np.float64(0.15313625610148493))]\n",
                        "\n",
                        "Top TF-IDF terms for Document 100:\n",
                        "[('puzzling', np.float64(0.5895736822928054)), ('besides', np.float64(0.5701606775653887)), ('dear', np.float64(0.41723085341143334)), ('oh', np.float64(0.3914563703242622))]\n"
                    ]
                }
            ],
            "source": [
                "cnt_vec = CountVectorizer()\n",
                "tf_vec = TfidfVectorizer()\n",
                "\n",
                "bow_matrix = cnt_vec.fit_transform(documents)\n",
                "tfidf_matrix = tf_vec.fit_transform(documents)\n",
                "\n",
                "print(f\"BoW matrix shape: {bow_matrix.shape}\")\n",
                "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
                "\n",
                "# Top 15 TF-IDF terms for 2 documents\n",
                "def get_top_tfidf(row_idx, vectorizer, matrix, top_n=15):\n",
                "    row = matrix.getrow(row_idx).toarray()[0]\n",
                "    indices = np.argsort(row)[::-1][:top_n]\n",
                "    feature_names = vectorizer.get_feature_names_out()\n",
                "    return [(feature_names[i], row[i]) for i in indices if row[i] > 0]\n",
                "\n",
                "idx1, idx2 = 10, 100\n",
                "print(f\"\\nTop TF-IDF terms for Document {idx1}:\")\n",
                "print(get_top_tfidf(idx1, tf_vec, tfidf_matrix))\n",
                "\n",
                "print(f\"\\nTop TF-IDF terms for Document {idx2}:\")\n",
                "print(get_top_tfidf(idx2, tf_vec, tfidf_matrix))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Interpretation:**\n",
                "The top TF-IDF terms for these documents effectively highlight the 'keywords' of each sentence. In TF-IDF, common words across the corpus receive lower scores, while words unique or central to the document (like 'rabbit' or 'queen' in specific contexts) get higher weights. This allows us to see what each document is practically 'about' without the noise of frequent words like 'said' or 'little'."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### B3. Similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Most similar documents: 1083 and 1084\n",
                        "Similarity Score: 1.0000000000000002\n",
                        "Doc 1083: soup evening beautiful soup\n",
                        "Doc 1084: soup evening beautiful soup\n",
                        "\n",
                        "--- Surprising Similarity Example ---\n",
                        "Doc 11: brave think home\n",
                        "Doc 550: alice beginning think creature get home\n",
                        "Similarity Score: 0.45394029907359607\n"
                    ]
                }
            ],
            "source": [
                "sim_matrix = cosine_similarity(tfidf_matrix)\n",
                "np.fill_diagonal(sim_matrix, 0) # Remove self-similarity\n",
                "\n",
                "max_sim_idx = np.unravel_index(np.argmax(sim_matrix), sim_matrix.shape)\n",
                "print(f\"Most similar documents: {max_sim_idx[0]} and {max_sim_idx[1]}\")\n",
                "print(f\"Similarity Score: {sim_matrix[max_sim_idx]}\")\n",
                "print(f\"Doc {max_sim_idx[0]}: {documents[max_sim_idx[0]]}\")\n",
                "print(f\"Doc {max_sim_idx[1]}: {documents[max_sim_idx[1]]}\")\n",
                "\n",
                "print(\"\\n--- Surprising Similarity Example ---\")\n",
                "potential_pairs = np.where((sim_matrix > 0.4) & (sim_matrix < 0.9))\n",
                "if len(potential_pairs[0]) > 0:\n",
                "    p1, p2 = potential_pairs[0][0], potential_pairs[1][0]\n",
                "    print(f\"Doc {p1}: {documents[p1]}\")\n",
                "    print(f\"Doc {p2}: {documents[p2]}\")\n",
                "    print(f\"Similarity Score: {sim_matrix[p1, p2]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Interpretation of Similarity:**\n",
                "The most similar pair generally shares significant rare keywords. A 'surprising' similarity might occur when two structurally different sentences both mention a rare character (like the 'Mock Turtle'), which counts heavily in the TF-IDF representation even if the rest of the sentence is distinct."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Part C - Word Embeddings (25%)\n",
                "\n",
                "**Goal:** Train Word2Vec embeddings and explore semantic similarity."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### C1. Prepare training data\n",
                "\n",
                "For embeddings, it is generally recommended to keep stopwords to preserve context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Sentence count for training: 1686\n"
                    ]
                }
            ],
            "source": [
                "embedding_data = [preprocess_text(s, remove_stopwords=False) for s in sentences if len(s) > 2]\n",
                "print(f\"Sentence count for training: {len(embedding_data)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### C2. Train Word2Vec\n",
                "\n",
                "I chose following hyperparameters:\n",
                "* `vector_size=100`: Balance between representation power and corpus size.\n",
                "* `window=5`: Standard context window.\n",
                "* `min_count=3`: Ensure words have enough context to learn an embedding.\n",
                "* `sg=1`: Skip-gram is used to better capture rare words in this smaller corpus.\n",
                "* `epochs=20`: Helps the model converge on small datasets."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Model training complete.\n"
                    ]
                }
            ],
            "source": [
                "from gensim.models import Word2Vec\n",
                "\n",
                "model = Word2Vec(\n",
                "    sentences=embedding_data, \n",
                "    vector_size=100, \n",
                "    window=5, \n",
                "    min_count=3, \n",
                "    sg=1, # Skip-gram\n",
                "    epochs=20\n",
                ")\n",
                "\n",
                "print(\"Model training complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### C3. Explore similarity"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Words similar to 'alice':\n",
                        "[('rather', 0.73460453748703), ('she', 0.7290487885475159), ('pleased', 0.7213724851608276), ('feeling', 0.7189539670944214), ('sharply', 0.7179637551307678), ('hastily', 0.7158926725387573), ('frightened', 0.7154080867767334), ('certainly', 0.7090147137641907), ('but', 0.7079842686653137), ('indignantly', 0.7030239105224609)]\n",
                        "\n",
                        "Words similar to 'queen':\n",
                        "[('executioner', 0.820957601070404), ('shouted', 0.8058800101280212), ('knave', 0.7945306897163391), ('turning', 0.7916951179504395), ('ground', 0.788692057132721), ('croquet', 0.782116174697876), ('heart', 0.7798267006874084), ('crown', 0.7781469821929932), ('pointing', 0.777019739151001), ('taken', 0.7761519551277161)]\n",
                        "\n",
                        "Words similar to 'hatter':\n",
                        "[('tea', 0.828532874584198), ('hare', 0.8276251554489136), ('executioner', 0.8128354549407959), ('interrupted', 0.7834770679473877), ('march', 0.7824484705924988), ('butter', 0.7797386050224304), ('pigeon', 0.7738587856292725), ('added', 0.772002100944519), ('pointing', 0.7666580677032471), ('master', 0.7658327221870422)]\n",
                        "\n",
                        "Words similar to 'mouse':\n",
                        "[('curious', 0.7787823677062988), ('o', 0.7565093040466309), ('sudden', 0.7476174831390381), ('tired', 0.7290216684341431), ('sharply', 0.7160797715187073), ('duchess', 0.7056976556777954), ('temper', 0.7032521963119507), ('frightened', 0.7011827826499939), ('severely', 0.6973901391029358), ('pleased', 0.6968719363212585)]\n",
                        "\n",
                        "Words similar to 'rabbit':\n",
                        "[('white', 0.8382276296615601), ('kid', 0.787730872631073), ('glove', 0.7734380960464478), ('hole', 0.7669762969017029), ('trumpet', 0.7622158527374268), ('jumping', 0.7348629236221313), ('read', 0.733935534954071), ('pair', 0.7318284511566162), ('fan', 0.7316511869430542), ('surprise', 0.7169572710990906)]\n"
                    ]
                }
            ],
            "source": [
                "target_words = ['alice', 'queen', 'hatter', 'mouse', 'rabbit']\n",
                "for word in target_words:\n",
                "    if word in model.wv:\n",
                "        print(f\"\\nWords similar to '{word}':\")\n",
                "        # Top 10 words as requested\n",
                "        print(model.wv.most_similar(word, topn=10))\n",
                "    else:\n",
                "        print(f\"Word '{word}' not in vocabulary.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Interpretation of Neighbors:**\n",
                "The neighbors make sense in the context of the corpus. For example, 'Alice' is often similar to descriptive words or other major characters she interacts with. 'Queen' and 'King' often cluster together. Since this is a small corpus, similarity often reflects co-occurrence rather than true synonymy, but characters from the same scenes correctly cluster together."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### C4. Analogies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "queen - king + alice = ?\n",
                        "Result: [('frightened', 0.737886369228363)]\n",
                        "white - black + rabbit = ?\n",
                        "Error: \"Key 'black' not present in vocabulary\"\n",
                        "hatter - alice + queen = ?\n",
                        "Result: [('executioner', 0.7436817288398743)]\n"
                    ]
                }
            ],
            "source": [
                "def try_analogy(a, b, c):\n",
                "    print(f\"{a} - {b} + {c} = ?\")\n",
                "    try:\n",
                "        result = model.wv.most_similar(positive=[a, c], negative=[b], topn=1)\n",
                "        print(f\"Result: {result}\")\n",
                "    except Exception as e:\n",
                "        print(f\"Error: {e}\")\n",
                "\n",
                "try_analogy('queen', 'king', 'alice')\n",
                "try_analogy('white', 'black', 'rabbit')\n",
                "try_analogy('hatter', 'alice', 'queen')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Interpretation of Analogies:**\n",
                "Analogy results on this small corpus may be weak. This is because Word2Vec requires millions of tokens to learn the complex relational geometries needed for perfect vector arithmetic. With only ~26k tokens, the vector space is sparsely populated, leading to some poor results. However, characters with strong contextual links still yield interesting neighbors."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
