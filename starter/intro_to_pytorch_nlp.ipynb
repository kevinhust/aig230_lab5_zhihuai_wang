{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4808d6a8",
      "metadata": {
        "id": "4808d6a8"
      },
      "source": [
        "\n",
        "# AIG 230 – Introduction to PyTorch for Natural Language Processing\n",
        "\n",
        "## Notebook 1: PyTorch Fundamentals for NLP\n",
        "\n",
        "This notebook is designed for **AIG 230 – Natural Language Processing (Seneca Polytechnic)**.\n",
        "It introduces PyTorch fundamentals that will be reused throughout the course for:\n",
        "\n",
        "- Sequence Models and RNNs  \n",
        "- Attention and Transformer Architectures  \n",
        "- Pretrained Models and Transfer Learning  \n",
        "- Named Entity Recognition  \n",
        "- Machine Translation and Seq2Seq  \n",
        "- Question Answering and Summarization  \n",
        "- Retrieval Augmented Generation and Text Search  \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b35a843",
      "metadata": {
        "id": "8b35a843"
      },
      "source": [
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, students will be able to:\n",
        "\n",
        "- Explain what PyTorch is and why it is used in NLP\n",
        "- Create and manipulate tensors\n",
        "- Understand devices (CPU vs GPU)\n",
        "- Explain automatic differentiation\n",
        "- Build and train a simple neural network\n",
        "- Read PyTorch code line by line\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "985bccc6",
      "metadata": {
        "id": "985bccc6"
      },
      "source": [
        "\n",
        "## 1. What is PyTorch and Why Do We Use It in NLP?\n",
        "\n",
        "PyTorch is a deep learning framework used to build, train, and deploy neural networks.\n",
        "\n",
        "In NLP, PyTorch is preferred because:\n",
        "\n",
        "- It supports dynamic computation graphs\n",
        "- It handles variable-length sequences naturally\n",
        "- It integrates with Hugging Face Transformers\n",
        "- It provides automatic differentiation\n",
        "\n",
        "Every model used later in this course is built on these fundamentals.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dda11c0",
      "metadata": {
        "id": "0dda11c0"
      },
      "source": [
        "\n",
        "### Student Checkpoint 1\n",
        "\n",
        "1. Why are dynamic computation graphs important for NLP?\n",
        "2. Name two NLP tasks from this course that require PyTorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8df9d78",
      "metadata": {
        "id": "b8df9d78"
      },
      "source": [
        "\n",
        "## 2. Installing and Importing PyTorch\n",
        "\n",
        "This notebook runs both **locally** and on **Google Colab**.\n",
        "Colab already includes PyTorch, but we include a safe install cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fc5cc2d",
      "metadata": {
        "id": "8fc5cc2d"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Install PyTorch (safe for Colab and local use)\n",
        "# In Colab, this will usually be skipped because PyTorch is preinstalled\n",
        "\n",
        "!pip install torch torchvision torchaudio --quiet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "83d1d145",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "83d1d145",
        "outputId": "3a332efc-8894-41b4-c047-b494f89731e1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.9.0+cu126'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "torch.__version__\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46ef5fbc",
      "metadata": {
        "id": "46ef5fbc"
      },
      "source": [
        "\n",
        "### Student Checkpoint 2\n",
        "\n",
        "1. What does the `torch` library provide?\n",
        "2. Why is checking the PyTorch version useful?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d100862e",
      "metadata": {
        "id": "d100862e"
      },
      "source": [
        "\n",
        "## 3. Tensors: The Core Data Structure\n",
        "\n",
        "Tensors are multi-dimensional arrays used to store:\n",
        "\n",
        "- Token IDs\n",
        "- Embeddings\n",
        "- Model weights\n",
        "- Batches of text sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "02d77b12",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02d77b12",
        "outputId": "6ad5b1a2-b7f6-4397-b090-5d2311a05b4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 2, 3])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "x = torch.tensor([1, 2, 3])\n",
        "x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "974b2269",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "974b2269",
        "outputId": "f4db8218-c784-4f1e-9532-350817d73938"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "x.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "812f8a7b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "812f8a7b",
        "outputId": "71ec3488-4c99-4956-be62-e9d892a334d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "x.dtype\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gg-no1qBLSu2",
      "metadata": {
        "id": "gg-no1qBLSu2"
      },
      "source": [
        "Tensors are PyTorch's most basic building block. Each tensor is a multi-dimensional matrix; for example, a 256x256 square image might be represented by a 3x256x256 tensor, where the first dimension represents color. Here's how to create a tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "kW7Qv5UtLSIi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW7Qv5UtLSIi",
        "outputId": "7331a7b8-40be-4289-fe23-2259d1e1fbfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[1, 2, 3], [4, 5, 6]]\n"
          ]
        }
      ],
      "source": [
        "list_of_lists = [\n",
        "  [1, 2, 3],\n",
        "  [4, 5, 6],\n",
        "]\n",
        "print(list_of_lists)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "Q0Pt0HfELP9y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0Pt0HfELP9y",
        "outputId": "abd3b376-b332-46d8-838c-542a02796233"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ],
      "source": [
        "data = torch.tensor(list_of_lists)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "qeNLShHLLbo2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeNLShHLLbo2",
        "outputId": "5caf1941-3991-4c75-8842-7035e9e3ccde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0, 1],\n",
            "        [2, 3],\n",
            "        [4, 5]])\n"
          ]
        }
      ],
      "source": [
        "# Initializing a tensor\n",
        "data = torch.tensor([\n",
        "                     [0, 1],\n",
        "                     [2, 3],\n",
        "                     [4, 5]\n",
        "                    ])\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "M_QZbMXWLhY1",
      "metadata": {
        "id": "M_QZbMXWLhY1"
      },
      "source": [
        "Each tensor has a data type: the major data types you'll need to worry about are floats (torch.float32) and integers (torch.int). You can specify the data type explicitly when you create the tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "zLqWXlAELhMC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLqWXlAELhMC",
        "outputId": "4030866b-9a51-4bf1-b21c-539d97fdbdd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 1.],\n",
            "        [2., 3.],\n",
            "        [4., 5.]])\n"
          ]
        }
      ],
      "source": [
        "# Initializing a tensor with an explicit data type\n",
        "# Notice the dots after the numbers, which specify that they're floats\n",
        "data = torch.tensor([\n",
        "                     [0, 1],\n",
        "                     [2, 3],\n",
        "                     [4, 5]\n",
        "                    ], dtype=torch.float32)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "73-TEKlcLgRc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73-TEKlcLgRc",
        "outputId": "fe43b34a-3589-482c-9b18-1118c2c3ccb4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1111, 1.0000],\n",
            "        [2.0000, 3.0000],\n",
            "        [4.0000, 5.0000]])\n"
          ]
        }
      ],
      "source": [
        "# Initializing a tensor with an explicit data type\n",
        "# Notice the dots after the numbers, which specify that they're floats\n",
        "data = torch.tensor([\n",
        "                     [0.11111111, 1],\n",
        "                     [2, 3],\n",
        "                     [4, 5]\n",
        "                    ], dtype=torch.float32)\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "6uT_C215Ls0p",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uT_C215Ls0p",
        "outputId": "f0620f82-2044-49ce-e145-5454d4ded18f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1111, 1.0000],\n",
            "        [2.0000, 3.0000],\n",
            "        [4.0000, 5.0000]])\n"
          ]
        }
      ],
      "source": [
        "# Initializing a tensor with an explicit data type\n",
        "# Notice the dots after the numbers, which specify that they're floats\n",
        "data = torch.tensor([\n",
        "                     [0.11111111, 1],\n",
        "                     [2, 3],\n",
        "                     [4, 5]\n",
        "                    ])\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1b789a9",
      "metadata": {
        "id": "a1b789a9"
      },
      "source": [
        "\n",
        "### Student Checkpoint 3\n",
        "\n",
        "1. What is the shape of a tensor representing one sentence with 10 tokens?\n",
        "2. Why do NLP models use tensors instead of Python lists?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74028c5d",
      "metadata": {
        "id": "74028c5d"
      },
      "source": [
        "\n",
        "## 4. Tensor Operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5bc8e1c0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bc8e1c0",
        "outputId": "21650725-b03e-4c69-e8be-4977147f0de4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([5., 7., 9.])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "a = torch.tensor([1.0, 2.0, 3.0])\n",
        "b = torch.tensor([4.0, 5.0, 6.0])\n",
        "\n",
        "a + b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5f2fdeb0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f2fdeb0",
        "outputId": "d6cbe8dd-c293-4b1b-a967-ff50337a8065"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 4., 10., 18.])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "a * b\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "8219ca0f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8219ca0f",
        "outputId": "bd9ef4a9-0873-4add-d01d-798c217157f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([17., 39.])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "M = torch.tensor([[1.0, 2.0],\n",
        "                  [3.0, 4.0]])\n",
        "v = torch.tensor([5.0, 6.0])\n",
        "\n",
        "torch.matmul(M, v)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60933398",
      "metadata": {
        "id": "60933398"
      },
      "source": [
        "\n",
        "### Student Checkpoint 4\n",
        "\n",
        "1. What is the difference between element-wise and matrix multiplication?\n",
        "2. Why is matrix multiplication essential for Transformers?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13471fb4",
      "metadata": {
        "id": "13471fb4"
      },
      "source": [
        "\n",
        "## 5. Devices: CPU vs GPU\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "1105c2f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1105c2f1",
        "outputId": "3410dcab-6c04-4643-92b7-b62928ab601c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "fd548e7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd548e7a",
        "outputId": "aa92bb15-8836-4ae2-a083-619ae6855685"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1, 2, 3], device='cuda:0')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "x = x.to(device)\n",
        "x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92c98bbe",
      "metadata": {
        "id": "92c98bbe"
      },
      "source": [
        "\n",
        "### Student Checkpoint 5\n",
        "\n",
        "1. What happens if data and model are on different devices?\n",
        "2. Why are GPUs critical for large language models?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ds-S-42L1b6",
      "metadata": {
        "id": "7ds-S-42L1b6"
      },
      "source": [
        "Utility functions also exist to create tensors with given shapes and contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "AApGvIONL0Zt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AApGvIONL0Zt",
        "outputId": "f8987ea0-96af-4751-8f94-280acf107929"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 0.],\n",
            "        [0., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "zeros = torch.zeros(2, 5)  # a tensor of all zeros\n",
        "print(zeros)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ue26M5Npqoe3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue26M5Npqoe3",
        "outputId": "e3765d5a-d0e4-42f2-c7fe-76fe1acaeb46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "ones = torch.ones(3, 4)   # a tensor of all ones\n",
        "print(ones)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "hdvaujtvqokI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdvaujtvqokI",
        "outputId": "902ecd3e-1474-45b3-af20-9e30acb00252"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
          ]
        }
      ],
      "source": [
        "rr = torch.arange(1, 10) # range from [1, 10)\n",
        "print(rr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "lK3q3LRHipHB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK3q3LRHipHB",
        "outputId": "bb0ad06e-f343-4b52-910d-bef89b5833dc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 3,  4,  5,  6,  7,  8,  9, 10, 11])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rr + 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "cDQ-v6AFiyco",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDQ-v6AFiyco",
        "outputId": "66ebd5ad-e90d-4b7a-ee85-0aa3657c8537"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([ 2,  4,  6,  8, 10, 12, 14, 16, 18])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rr * 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "OFcjwshvi3pC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFcjwshvi3pC",
        "outputId": "585cc996-0844-4fce-c718-9b7896146d7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A is tensor([[1, 2],\n",
            "        [2, 3],\n",
            "        [4, 5]])\n",
            "B is tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n",
            "The product is tensor([[11, 14, 17, 20],\n",
            "        [17, 22, 27, 32],\n",
            "        [29, 38, 47, 56]])\n",
            "The other product is tensor([[11, 14, 17, 20],\n",
            "        [17, 22, 27, 32],\n",
            "        [29, 38, 47, 56]])\n"
          ]
        }
      ],
      "source": [
        "a = torch.tensor([[1, 2], [2, 3], [4, 5]])      # (3, 2)\n",
        "b = torch.tensor([[1, 2, 3, 4], [5, 6, 7, 8]])  # (2, 4)\n",
        "\n",
        "print(\"A is\", a)\n",
        "print(\"B is\", b)\n",
        "print(\"The product is\", a.matmul(b)) #(3, 4)\n",
        "print(\"The other product is\", a @ b) # +, -, *, @"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SCS5z1lip9lq",
      "metadata": {
        "id": "SCS5z1lip9lq"
      },
      "source": [
        "The **shape** of a matrix (which can be accessed by `.shape`) is defined as the dimensions of the matrix. Here's some examples:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "fMptIpZkq0da",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMptIpZkq0da",
        "outputId": "00b836b7-374a-4b55-ae69-2f5b714fb48e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3])\n",
            "tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ]
        }
      ],
      "source": [
        "matr_2d = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "print(matr_2d.shape)\n",
        "print(matr_2d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "aWjD7WjPqC6t",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWjD7WjPqC6t",
        "outputId": "63ed4179-0403-4b08-afa0-05d4215acd62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 1,  2,  3,  4],\n",
            "         [-2,  5,  6,  9]],\n",
            "\n",
            "        [[ 5,  6,  7,  2],\n",
            "         [ 8,  9, 10,  4]],\n",
            "\n",
            "        [[-3,  2,  2,  1],\n",
            "         [ 4,  6,  5,  9]]])\n",
            "torch.Size([3, 2, 4])\n"
          ]
        }
      ],
      "source": [
        "matr_3d = torch.tensor([[[1, 2, 3, 4], [-2, 5, 6, 9]], [[5, 6, 7, 2], [8, 9, 10, 4]], [[-3, 2, 2, 1], [4, 6, 5, 9]]])\n",
        "print(matr_3d)\n",
        "print(matr_3d.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7wKqP85rrF-P",
      "metadata": {
        "id": "7wKqP85rrF-P"
      },
      "source": [
        "**Reshaping** tensors can be used to make batch operations easier (more on that later), but be careful that the data is reshaped in the order you expect:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "HmUcqHYUrMu1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmUcqHYUrMu1",
        "outputId": "16bdb31e-2fc0-464b-e6c2-50df3ac6c71a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The shape is currently torch.Size([15])\n",
            "The contents are currently tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
            "\n",
            "After reshaping, the shape is currently torch.Size([5, 3])\n",
            "The contents are currently tensor([[ 1,  2,  3],\n",
            "        [ 4,  5,  6],\n",
            "        [ 7,  8,  9],\n",
            "        [10, 11, 12],\n",
            "        [13, 14, 15]])\n"
          ]
        }
      ],
      "source": [
        "rr = torch.arange(1, 16)\n",
        "print(\"The shape is currently\", rr.shape)\n",
        "print(\"The contents are currently\", rr)\n",
        "print()\n",
        "rr = rr.view(5, 3)\n",
        "print(\"After reshaping, the shape is currently\", rr.shape)\n",
        "print(\"The contents are currently\", rr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GaykBuhoou3M",
      "metadata": {
        "id": "GaykBuhoou3M"
      },
      "source": [
        "Finally, you can also inter-convert tensors with **NumPy arrays**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ppYiPnlko1Ci",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ppYiPnlko1Ci",
        "outputId": "7ec347e2-7f98-4c1e-ec8d-1660c8a4187b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is a torch.tensor tensor([[1, 0, 5]])\n",
            "This is a np.ndarray [[1 0 5]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "# numpy.ndarray --> torch.Tensor:\n",
        "arr = np.array([[1, 0, 5]])\n",
        "data = torch.tensor(arr)\n",
        "print(\"This is a torch.tensor\", data)\n",
        "\n",
        "# torch.Tensor --> numpy.ndarray:\n",
        "new_arr = data.numpy()\n",
        "print(\"This is a np.ndarray\", new_arr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hyv1l431q9yA",
      "metadata": {
        "id": "hyv1l431q9yA"
      },
      "source": [
        "One of the reasons why we use **tensors** is *vectorized operations*: operations that be conducted in parallel over a particular dimension of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "Kas2MEFDsJWk",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kas2MEFDsJWk",
        "outputId": "aef23fe6-973a-423c-e7e8-9c7abc2dbc29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data is: tensor([[ 1.,  2.,  3.,  4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11., 12., 13., 14.],\n",
            "        [15., 16., 17., 18., 19., 20., 21.],\n",
            "        [22., 23., 24., 25., 26., 27., 28.],\n",
            "        [29., 30., 31., 32., 33., 34., 35.]])\n",
            "Taking the sum over rows:\n",
            "tensor([ 28.,  77., 126., 175., 224.])\n",
            "Taking thep sum over columns:\n",
            "tensor([ 75.,  80.,  85.,  90.,  95., 100., 105.])\n",
            "Taking the stdev over rows:\n",
            "tensor([2.1602, 2.1602, 2.1602, 2.1602, 2.1602])\n"
          ]
        }
      ],
      "source": [
        "data = torch.arange(1, 36, dtype=torch.float32).reshape(5, 7)\n",
        "print(\"Data is:\", data)\n",
        "\n",
        "# We can perform operations like *sum* over each row...\n",
        "print(\"Taking the sum over rows:\")\n",
        "print(data.sum(dim=1)) #(5,)\n",
        "\n",
        "# or over each column.\n",
        "print(\"Taking thep sum over columns:\")\n",
        "print(data.sum(dim=0)) #(7,)\n",
        "\n",
        "# Other operations are available:\n",
        "print(\"Taking the stdev over rows:\")\n",
        "print(data.std(dim=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "g1ByKJL_WYWv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1ByKJL_WYWv",
        "outputId": "1f25a75b-3aad-4ee0-e764-94d2977d7dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[1., 2., 3.],\n",
            "         [4., 5., 6.]]])\n",
            "tensor([5., 7., 9.])\n",
            "torch.Size([3])\n"
          ]
        }
      ],
      "source": [
        "data = torch.arange(1, 7, dtype=torch.float32).reshape(1, 2, 3)\n",
        "print(data)\n",
        "print(data.sum(dim=0).sum(dim=0))\n",
        "print(data.sum(dim=0).sum(dim=0).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "NPRy-xtuk2tK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPRy-xtuk2tK",
        "outputId": "909fdff9-58d5-4e7e-b578-88362537eb0e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(21.)"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V7BMktFFAkRA",
      "metadata": {
        "id": "V7BMktFFAkRA"
      },
      "source": [
        "**Indexing**\n",
        "\n",
        "You can access arbitrary elements of a tensor using the `[]` operator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "fRJN7ovWDsKV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRJN7ovWDsKV",
        "outputId": "74c9ccf5-82d2-42e4-f372-f6617cc044ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.,  2.],\n",
              "         [ 3.,  4.]],\n",
              "\n",
              "        [[ 5.,  6.],\n",
              "         [ 7.,  8.]],\n",
              "\n",
              "        [[ 9., 10.],\n",
              "         [11., 12.]]])"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize an example tensor\n",
        "x = torch.Tensor([\n",
        "                  [[1, 2], [3, 4]],\n",
        "                  [[5, 6], [7, 8]],\n",
        "                  [[9, 10], [11, 12]]\n",
        "                 ])\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "M67ZiOF1Heyc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M67ZiOF1Heyc",
        "outputId": "0990ba29-eeb9-4107-8d38-088a31c58c08"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 2])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "guXKE7m8AX1K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guXKE7m8AX1K",
        "outputId": "17114205-a2fc-4586-e705-d6da33d3c720"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Access the 0th element, which is the first row\n",
        "x[0] # Equivalent to x[0, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "zn4pW2rkmXuj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn4pW2rkmXuj",
        "outputId": "4f921d8d-8438-4c85-a5f5-50d07f0d5d83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.,  2.],\n",
              "        [ 5.,  6.],\n",
              "        [ 9., 10.]])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g8m8EyVvES4-",
      "metadata": {
        "id": "g8m8EyVvES4-"
      },
      "source": [
        "We can also index into multiple dimensions with `:`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "2Z6GFUcuEL85",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z6GFUcuEL85",
        "outputId": "5769948b-53e0-4cf5-853c-260d43212893"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 5., 9.])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the top left element of each element in our tensor\n",
        "x[:, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "TRkMhiJNnWAt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRkMhiJNnWAt",
        "outputId": "31fb3803-f9b2-4a26-fb7f-08c26384fbb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 1.,  2.],\n",
              "         [ 3.,  4.]],\n",
              "\n",
              "        [[ 5.,  6.],\n",
              "         [ 7.,  8.]],\n",
              "\n",
              "        [[ 9., 10.],\n",
              "         [11., 12.]]])"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[:, :, :]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rm8vc3nuXaEw",
      "metadata": {
        "id": "Rm8vc3nuXaEw"
      },
      "source": [
        "We can also access arbitrary elements in each dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "R4xl6CW3RrEw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4xl6CW3RrEw",
        "outputId": "add8de02-4764-4c8e-a4dc-a99361c32b0b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[1., 2.],\n",
              "         [3., 4.]],\n",
              "\n",
              "        [[1., 2.],\n",
              "         [3., 4.]],\n",
              "\n",
              "        [[5., 6.],\n",
              "         [7., 8.]],\n",
              "\n",
              "        [[5., 6.],\n",
              "         [7., 8.]]])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's access the 0th and 1st elements, each twice\n",
        "# same as stacking x[0], x[0], x[1], x[1]\n",
        "i = torch.tensor([0, 0, 1, 1])\n",
        "x[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "A3QYZ8k7Wvqp",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3QYZ8k7Wvqp",
        "outputId": "864445ab-dcea-49e4-da4d-04a6c3113c96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 5.,  6.],\n",
              "        [ 9., 10.]])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Let's access the 0th elements of the 1st and 2nd elements\n",
        "\n",
        "i = torch.tensor([1, 2])\n",
        "j = torch.tensor([0])\n",
        "x[i, j]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WAELXC--IHS7",
      "metadata": {
        "id": "WAELXC--IHS7"
      },
      "source": [
        "We can get a `Python` scalar value from a tensor with `item()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "BM-ZujN2IGaQ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BM-ZujN2IGaQ",
        "outputId": "b561df69-b1a6-4083-8d3a-4b672f681c8d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0, 0, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "6NwxK7d_Ycgs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NwxK7d_Ycgs",
        "outputId": "86905d36-9668-4fae-c36a-509a5db0be38"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x[0, 0, 0].item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cc4ad1d",
      "metadata": {
        "id": "8cc4ad1d"
      },
      "source": [
        "\n",
        "## 6. Automatic Differentiation (Autograd)\n",
        "\n",
        "Pytorch is well-known for its automatic differentiation feature. We can call the backward() method to ask PyTorch to calculate the gradients, which are then stored in the grad attribute.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "f18128fc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f18128fc",
        "outputId": "a198d187-4f62-4e8f-a2be-0a8c4a700cb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(7.)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "x = torch.tensor(2.0, requires_grad=True)\n",
        "y = x ** 2 + 3 * x + 1\n",
        "y.backward()\n",
        "x.grad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-oEvBJHWfn8H",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-oEvBJHWfn8H",
        "outputId": "fa65c16c-8041-4ae0-a79d-7ffbd31e2e65"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "None\n"
          ]
        }
      ],
      "source": [
        "# Create an example tensor\n",
        "# requires_grad parameter tells PyTorch to store gradients\n",
        "x = torch.tensor([2.], requires_grad=True)\n",
        "\n",
        "# Print the gradient if it is calculated\n",
        "# Currently None since x is a scalar\n",
        "pp.pprint(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DTJazZXkgthP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTJazZXkgthP",
        "outputId": "8730c813-ae43-4316-f84d-68d65ca0bd9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([12.])\n"
          ]
        }
      ],
      "source": [
        "# Calculating the gradient of y with respect to x\n",
        "y = x * x * 3 # 3x^2\n",
        "y.backward()\n",
        "pp.pprint(x.grad) # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Hqc2oM3iV6a",
      "metadata": {
        "id": "3Hqc2oM3iV6a"
      },
      "source": [
        "Let's run backprop from a different tensor again to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K--Az0Xiic_z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K--Az0Xiic_z",
        "outputId": "07e848bc-f87a-476f-fc57-44a74be50874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([24.])\n"
          ]
        }
      ],
      "source": [
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "pp.pprint(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4tIa4dw6Tmar",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tIa4dw6Tmar",
        "outputId": "27935662-bf85-4421-9829-c43bf9d651a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([12.])\n"
          ]
        }
      ],
      "source": [
        "x.grad = None\n",
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "# y = x * x * 3\n",
        "pp.pprint(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pbyr-P4rdJ9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pbyr-P4rdJ9b",
        "outputId": "7ad721e9-ef65-4309-b765-c63f9402a814"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([24.])\n"
          ]
        }
      ],
      "source": [
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "# y = x * x * 3\n",
        "pp.pprint(x.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WpvRq_wUdM5y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpvRq_wUdM5y",
        "outputId": "34aaf32c-5ac9-4c19-8dfe-c34fe8f05f6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([36.])\n"
          ]
        }
      ],
      "source": [
        "z = x * x * 3 # 3x^2\n",
        "z.backward()\n",
        "# y = x * x * 3\n",
        "pp.pprint(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HhjPkiE6i7ja",
      "metadata": {
        "id": "HhjPkiE6i7ja"
      },
      "source": [
        "We can see that the `x.grad` is updated to be the sum of the gradients calculated so far. When we run backprop in a neural network, we sum up all the gradients for a particular neuron before making an update. This is exactly what is happening here! This is also the reason why we need to run `zero_grad()` in every training iteration (more on this later). Otherwise our gradients would keep building up from one training iteration to the other, which would cause our updates to be wrong."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w-9I0zZOKtXr",
      "metadata": {
        "id": "w-9I0zZOKtXr"
      },
      "source": [
        "## Customized Backward Function\n",
        "In some rare cases, you might want to design your own operators, or calculate higher order gradients that are not supported by Pytorch. In these cases you can define your own function with customized forward & backward computation. However, keep in mind that always check if something is already implemented by Pytorch (which is very likely) before customizing your own forward & backward computation. See more at https://pytorch.org/docs/stable/notes/extending.html."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pYLWqKIoaOyd",
      "metadata": {
        "id": "pYLWqKIoaOyd"
      },
      "source": [
        "## Neural Network Module\n",
        "\n",
        "So far we have looked into the tensors, their properties and basic operations on tensors. These are especially useful to get familiar with if we are building the layers of our network from scratch. We will utilize these in Assignment 2, but moving forward, we will use predefined blocks in the `torch.nn` module of `PyTorch`. We will then put together these blocks to create complex networks. Let's start by importing this module with an alias so that we don't have to type `torch` every time we use it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qUmrDpbhV4Tn",
      "metadata": {
        "id": "qUmrDpbhV4Tn"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "joGvRWjEbak0",
      "metadata": {
        "id": "joGvRWjEbak0"
      },
      "source": [
        "### **Linear Layer**\n",
        "We can use `nn.Linear(H_in, H_out)` to create a a linear layer. This will take a matrix of `(N, *, H_in)` dimensions and output a matrix of `(N, *, H_out)`. The `*` denotes that there could be arbitrary number of dimensions in between. The linear layer performs the operation `Ax+b`, where `A` and `b` are initialized randomly. If we don't want the linear layer to learn the bias parameters, we can initialize our layer with `bias=False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6XfnKI4-a5j9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XfnKI4-a5j9",
        "outputId": "ebce3d47-b720-4ff4-a279-83c2ab3513cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.8810,  0.3769],\n",
              "         [-0.8810,  0.3769],\n",
              "         [-0.8810,  0.3769]],\n",
              "\n",
              "        [[-0.8810,  0.3769],\n",
              "         [-0.8810,  0.3769],\n",
              "         [-0.8810,  0.3769]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 180,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create the inputs\n",
        "input = torch.ones(2,3,4)\n",
        "# N* H_in -> N*H_out\n",
        "\n",
        "\n",
        "# Make a linear layers transforming N,*,H_in dimensinal inputs to N,*,H_out\n",
        "# dimensional outputs\n",
        "linear = nn.Linear(4, 2)\n",
        "linear_output = linear(input)\n",
        "linear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ao4XddjxeBVJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao4XddjxeBVJ",
        "outputId": "088e2c85-511a-4508-c392-f38e38abf15f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 3, 10, 11, 2])"
            ]
          },
          "execution_count": 178,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear_output.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0_9XKtAFYpdI",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0_9XKtAFYpdI",
        "outputId": "992db13a-f581-4cfd-f840-fa49778e696e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[-0.1443,  0.2130,  0.2116, -0.4267],\n",
              "         [-0.3379,  0.2243,  0.3289, -0.2484]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.4608,  0.0073], requires_grad=True)]"
            ]
          },
          "execution_count": 179,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(linear.parameters()) # Ax + b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FsyBaqzZppS8",
      "metadata": {
        "id": "FsyBaqzZppS8"
      },
      "outputs": [],
      "source": [
        "# Data of shape [batch_size, feature_dim] # 4\n",
        "# [batch_size, output_dim] # 2\n",
        "\n",
        "# linear layer of shape (feature_dim, output_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jAXCCu9keUlW",
      "metadata": {
        "id": "jAXCCu9keUlW"
      },
      "source": [
        "### **Other Module Layers**\n",
        "There are several other preconfigured layers in the `nn` module. Some commonly used examples are `nn.Conv2d`, `nn.ConvTranspose2d`, `nn.BatchNorm1d`, `nn.BatchNorm2d`, `nn.Upsample` and `nn.MaxPool2d` among many others. We will learn more about these as we progress in the course. For now, the only important thing to remember is that we can treat each of these layers as plug and play components: we will be providing the required dimensions and `PyTorch` will take care of setting them up."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yslDOK66fYWn",
      "metadata": {
        "id": "yslDOK66fYWn"
      },
      "source": [
        "### **Activation Function Layer**\n",
        "We can also use the `nn` module to apply activations functions to our tensors. Activation functions are used to add non-linearity to our network. Some examples of activations functions are `nn.ReLU()`, `nn.Sigmoid()` and `nn.LeakyReLU()`. Activation functions operate on each element seperately, so the shape of the tensors we get as an output are the same as the ones we pass in."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IrJP5CveeOON",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrJP5CveeOON",
        "outputId": "b899dc1f-b2d8-43f5-8338-c0606271e3b8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-0.8810,  0.3769],\n",
              "         [-0.8810,  0.3769],\n",
              "         [-0.8810,  0.3769]],\n",
              "\n",
              "        [[-0.8810,  0.3769],\n",
              "         [-0.8810,  0.3769],\n",
              "         [-0.8810,  0.3769]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "execution_count": 181,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "linear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "W9v5FjQtd4Ck",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9v5FjQtd4Ck",
        "outputId": "2cc3cdf3-8153-4a43-d642-420440956523"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0.2930, 0.5931],\n",
              "         [0.2930, 0.5931],\n",
              "         [0.2930, 0.5931]],\n",
              "\n",
              "        [[0.2930, 0.5931],\n",
              "         [0.2930, 0.5931],\n",
              "         [0.2930, 0.5931]]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 182,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sigmoid = nn.Sigmoid()\n",
        "output = sigmoid(linear_output)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RiYTthJwhEYT",
      "metadata": {
        "id": "RiYTthJwhEYT"
      },
      "source": [
        "### **Putting the Layers Together**\n",
        "So far we have seen that we can create layers and pass the output of one as the input of the next. Instead of creating intermediate tensors and passing them around, we can use `nn.Sequentual`, which does exactly that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xtJeOqLxhBLY",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtJeOqLxhBLY",
        "outputId": "054d47ee-ae85-4cc7-8cd7-07e446232f84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[0.6822, 0.5056],\n",
              "         [0.6822, 0.5056],\n",
              "         [0.6822, 0.5056]],\n",
              "\n",
              "        [[0.6822, 0.5056],\n",
              "         [0.6822, 0.5056],\n",
              "         [0.6822, 0.5056]]], grad_fn=<SigmoidBackward0>)"
            ]
          },
          "execution_count": 183,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block = nn.Sequential(\n",
        "    nn.Linear(4, 2),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "input = torch.ones(2,3,4)\n",
        "output = block(input)\n",
        "output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c449ca53",
      "metadata": {
        "id": "c449ca53"
      },
      "source": [
        "\n",
        "### Student Checkpoint 6\n",
        "\n",
        "1. Why are gradients needed in training?\n",
        "2. What does `backward()` compute?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94493dae",
      "metadata": {
        "id": "94493dae"
      },
      "source": [
        "\n",
        "## 7. Building a Simple Neural Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8040b13a",
      "metadata": {
        "id": "8040b13a"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(10, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6520c180",
      "metadata": {
        "id": "6520c180"
      },
      "source": [
        "\n",
        "### Student Checkpoint 7\n",
        "\n",
        "1. What does `nn.Module` represent?\n",
        "2. Why must the `forward` method be defined?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a82a3cf5",
      "metadata": {
        "id": "a82a3cf5"
      },
      "source": [
        "\n",
        "## 8. Loss Functions and Optimizers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9fd5296",
      "metadata": {
        "id": "a9fd5296"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = SimpleModel().to(device)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa7df605",
      "metadata": {
        "id": "fa7df605"
      },
      "source": [
        "\n",
        "### Student Checkpoint 8\n",
        "\n",
        "1. What does the loss function measure?\n",
        "2. Why is Adam commonly used in NLP?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b209c9",
      "metadata": {
        "id": "59b209c9"
      },
      "source": [
        "\n",
        "## 9. Training Loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ecbbe70",
      "metadata": {
        "id": "0ecbbe70"
      },
      "outputs": [],
      "source": [
        "\n",
        "x = torch.randn(5, 10).to(device)\n",
        "y = torch.randn(5, 1).to(device)\n",
        "\n",
        "for epoch in range(3):\n",
        "    optimizer.zero_grad()\n",
        "    predictions = model(x)\n",
        "    loss = loss_fn(predictions, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5eb6e00e",
      "metadata": {
        "id": "5eb6e00e"
      },
      "source": [
        "\n",
        "### Student Checkpoint 9\n",
        "\n",
        "1. Why are gradients reset every iteration?\n",
        "2. Which line updates the model weights?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7058518e",
      "metadata": {
        "id": "7058518e"
      },
      "source": [
        "\n",
        "## 10. How This Maps to NLP Models\n",
        "\n",
        "| PyTorch Concept | NLP Usage |\n",
        "|----------------|----------|\n",
        "| Tensor | Token IDs, embeddings |\n",
        "| Linear Layer | Feedforward blocks |\n",
        "| Autograd | Backpropagation |\n",
        "| Optimizer | Fine-tuning |\n",
        "| Training Loop | RNNs, Transformers, LLMs |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AAVs0tXaMpJ-",
      "metadata": {
        "id": "AAVs0tXaMpJ-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1c066d24",
      "metadata": {
        "id": "1c066d24"
      },
      "source": [
        "\n",
        "### Final Reflection\n",
        "\n",
        "1. Which PyTorch concept is still unclear?\n",
        "2. Which upcoming NLP topic will rely most on these fundamentals?\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aig230-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
